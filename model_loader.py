# Revised model_loader.py (unchanged as sync)
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import wandb
import os
import sys

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = None
model = None

def load_model_sync():
    global tokenizer, model

    try:
        print("üîÑ Initializing Weights & Biases...")
        run = wandb.init(project="inference", job_type="deploy", anonymous="allow")
    except Exception as e:
        print(f"‚ùå wandb.init() failed: {e}")
        sys.exit(1)

    try:
        print("üì¶ Using model artifact from wandb...")
        artifact = run.use_artifact(
            "medoxz543-zewail-city-of-science-and-technology/bertweet-lora-bayes-v2/final_model:v5",
            type="model"
        )
        model_dir = artifact.download()
    except Exception as e:
        print(f"‚ùå Failed to load or download artifact: {e}")
        sys.exit(1)

    try:
        print("üî† Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base")
    except Exception as e:
        print(f"‚ùå Failed to load tokenizer: {e}")
        sys.exit(1)

    try:
        print("üß† Loading model weights...")
        model = AutoModelForSequenceClassification.from_pretrained(model_dir).to(DEVICE)
        model.eval()
        print("‚úÖ Model is ready!")
    except Exception as e:
        print(f"‚ùå Failed to load model weights: {e}")
        sys.exit(1)
